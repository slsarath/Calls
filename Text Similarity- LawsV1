#!/usr/bin/env python3
"""
lrr_control_matcher_full.py

Comprehensive single-file solution to match LRR text to control descriptions at scale.
Features:
 - sentence-transformers for semantic embeddings (recommended for large text)
 - optional FAISS or hnswlib ANN index for fast nearest-neighbor search
 - TF-IDF + SVD fallback if sentence-transformers not available
 - Heuristics: n-grams, Jaccard, fuzzy (rapidfuzz), regex citation matching, phrase matching
 - Candidate retrieval by ANN then reranking using composite scores (faster than reranking full corpus)
 - Save/load embeddings and indexes
 - CLI: preprocess/build-index/match
"""

from typing import List, Dict, Optional, Tuple
import re
import os
import sys
import math
import json
import argparse
import numpy as np
import pandas as pd
from difflib import SequenceMatcher

# Optional libraries
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except Exception:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    import faiss
    FAISS_AVAILABLE = True
except Exception:
    FAISS_AVAILABLE = False

try:
    import hnswlib
    HNSW_AVAILABLE = True
except Exception:
    HNSW_AVAILABLE = False

try:
    from rapidfuzz import fuzz
    RAPIDFUZZ_AVAILABLE = True
except Exception:
    RAPIDFUZZ_AVAILABLE = False

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except Exception:
    TQDM_AVAILABLE = False

# sklearn for fallback embedding and utilities
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# ---------------------------
# Text preprocessing utilities
# ---------------------------
def preprocess_text(text: Optional[str]) -> str:
    if text is None:
        return ""
    if isinstance(text, float) and math.isnan(text):
        return ""
    s = str(text)
    s = s.replace("\r", " ").replace("\n", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize(text: str) -> List[str]:
    return [t for t in re.findall(r"\w+", text.lower()) if t]

def ngrams(tokens: List[str], n: int) -> List[str]:
    if len(tokens) < n:
        return []
    return [" ".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

# ---------------------------
# Embeddings helpers
# ---------------------------
def get_st_model(model_name: str = "all-MiniLM-L6-v2"):
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise ImportError("sentence-transformers not installed. pip install -U sentence-transformers")
    model = SentenceTransformer(model_name)
    return model

def encode_texts_st(model, texts: List[str], batch_size: int = 64, show_progress: bool = True) -> np.ndarray:
    """
    Encode texts into embeddings using a SentenceTransformer model with batching.
    """
    if show_progress and TQDM_AVAILABLE:
        iterator = tqdm(range(0, len(texts), batch_size), desc="Encoding", file=sys.stdout)
    else:
        iterator = range(0, len(texts), batch_size)
    embs = []
    for i in iterator:
        batch = texts[i:i+batch_size]
        emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        embs.append(emb)
    if len(embs) == 0:
        return np.zeros((0, 0), dtype=float)
    return np.vstack(embs)

def tfidf_svd_embeddings(corpus: List[str], n_components: int = 200) -> Tuple[np.ndarray, TfidfVectorizer, TruncatedSVD]:
    vec = TfidfVectorizer(max_df=0.9, min_df=1, ngram_range=(1,2))
    X = vec.fit_transform(corpus)
    n_comp = min(n_components, max(1, X.shape[1]-1))
    svd = TruncatedSVD(n_components=n_comp, random_state=42)
    Xr = svd.fit_transform(X)
    return Xr, vec, svd

# ---------------------------
# Similarity measures
# ---------------------------
def cosine_similarity_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    if A.size == 0 or B.size == 0:
        return np.zeros((A.shape[0], B.shape[0]))
    A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)
    B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)
    A_norm = np.nan_to_num(A_norm)
    B_norm = np.nan_to_num(B_norm)
    return np.matmul(A_norm, B_norm.T)

def jaccard_tokens(a: str, b: str) -> float:
    ta = set(tokenize(a)); tb = set(tokenize(b))
    if not ta and not tb:
        return 0.0
    return len(ta & tb) / len(ta | tb)

def ngram_overlap_score(a: str, b: str, max_n: int = 3) -> float:
    ta = tokenize(a); tb = tokenize(b)
    scores = []
    for n in range(1, max_n+1):
        ga = set(ngrams(ta, n)); gb = set(ngrams(tb, n))
        if not ga and not gb:
            scores.append(0.0)
        else:
            scores.append(len(ga & gb) / max(1, len(ga | gb)))
    weights = [0.2, 0.3, 0.5][:len(scores)]
    return float(np.dot(scores, weights))

def fuzzy_ratio(a: str, b: str) -> float:
    if RAPIDFUZZ_AVAILABLE:
        return fuzz.token_sort_ratio(a, b) / 100.0
    return SequenceMatcher(None, a, b).ratio()

def regex_section_match_score(a: str, b: str) -> float:
    pattern = r"\b\d+\s*CFR\s*\d+(?:\.\d+)*(?:\([a-z0-9]+\))*\b|\b\d+(?:\.\d+){1,}\b"
    refs_a = set(re.findall(pattern, a, flags=re.IGNORECASE))
    refs_b = set(re.findall(pattern, b, flags=re.IGNORECASE))
    if not refs_a or not refs_b:
        return 0.0
    return len(refs_a & refs_b) / len(refs_a)

def phrase_match_count(a: str, b: str, min_phrase_len: int = 3) -> float:
    ta = tokenize(a); tb = tokenize(b)
    if len(ta) < min_phrase_len or len(tb) < min_phrase_len:
        return 0.0
    matches = 0; total = 0
    for n in range(min_phrase_len, min(6, len(ta))+1):
        sa = set(ngrams(ta, n)); sb = set(ngrams(tb, n))
        if sa or sb:
            common = sa & sb
            matches += len(common)
            total += max(1, len(sa | sb))
    return 0.0 if total == 0 else matches / total

# ---------------------------
# ANN index utilities (faiss/hnswlib/brute)
# ---------------------------
class ANNIndex:
    """
    Wrapper to build/use FAISS or hnswlib ANN indices. If neither present uses brute force.
    Normalizes embeddings for cosine similarity / inner product usage.
    """
    def __init__(self, method_preference: str = "auto"):
        # method_preference: "auto", "faiss", "hnsw", "brute"
        self.method = None
        self.index = None
        self.dim = None
        self.method_preference = method_preference

    def choose_method(self):
        if self.method_preference == "faiss" and FAISS_AVAILABLE:
            return "faiss"
        if self.method_preference == "hnsw" and HNSW_AVAILABLE:
            return "hnsw"
        if self.method_preference == "auto":
            if FAISS_AVAILABLE:
                return "faiss"
            if HNSW_AVAILABLE:
                return "hnsw"
        return "brute"

    def build(self, embeddings: np.ndarray, ef_construction: int = 200, M: int = 16, use_gpu: bool = False):
        """
        embeddings: numpy array shape (n_items, dim), float32
        """
        if embeddings.size == 0:
            raise ValueError("Empty embeddings passed to ANNIndex.build")
        n, d = embeddings.shape
        self.dim = d
        chosen = self.choose_method()
        self.method = chosen

        # normalize for cosine similarity
        # for FAISS we use inner product on L2-normalized vectors
        embs = embeddings.astype(np.float32)
        norms = np.linalg.norm(embs, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        embs = embs / norms

        if chosen == "faiss":
            # IndexFlatIP for inner product; normalized vectors => cosine
            index = faiss.IndexFlatIP(d)
            if use_gpu:
                try:
                    res = faiss.StandardGpuResources()
                    index = faiss.index_cpu_to_gpu(res, 0, index)
                except Exception:
                    # fallback to cpu
                    pass
            index.add(embs)
            self.index = index
        elif chosen == "hnsw":
            p = hnswlib.Index(space='cosine', dim=d)
            p.init_index(max_elements=n, ef_construction=ef_construction, M=M)
            p.add_items(embs, np.arange(n))
            p.set_ef(ef_construction)  # search param; users can tune via set_ef()
            self.index = p
        else:
            # brute force: store normalized embeddings and compute dot-product on query
            self.index = {"embs": embs}
        return self.method

    def save(self, path_prefix: str):
        os.makedirs(os.path.dirname(path_prefix) or ".", exist_ok=True)
        if self.method == "faiss":
            faiss.write_index(self.index, path_prefix + ".faiss")
        elif self.method == "hnsw":
            self.index.save_index(path_prefix + ".hnsw")
        else:
            # brute: save numpy array
            np.save(path_prefix + ".npy", self.index["embs"])
        # record metadata
        meta = {"method": self.method, "dim": self.dim}
        with open(path_prefix + ".meta.json", "w") as f:
            json.dump(meta, f)

    def load(self, path_prefix: str):
        meta = json.load(open(path_prefix + ".meta.json", "r"))
        self.method = meta.get("method", None)
        self.dim = meta.get("dim", None)
        if self.method == "faiss":
            self.index = faiss.read_index(path_prefix + ".faiss")
        elif self.method == "hnsw":
            p = hnswlib.Index(space='cosine', dim=self.dim)
            p.load_index(path_prefix + ".hnsw")
            self.index = p
        else:
            embs = np.load(path_prefix + ".npy")
            self.index = {"embs": embs}

    def query(self, q_embs: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        q_embs: (m, dim)
        returns (ids, scores) both shape (m, top_k)
        Scores normalized so higher is better (cosine similarity). For hnswlib distances are cosine distances -> convert
        """
        if q_embs.size == 0:
            return np.zeros((0, top_k), dtype=int), np.zeros((0, top_k), dtype=float)

        # normalize query embeddings
        q = q_embs.astype(np.float32)
        qnorms = np.linalg.norm(q, axis=1, keepdims=True)
        qnorms[qnorms == 0] = 1.0
        q = q / qnorms

        if self.method == "faiss":
            D, I = self.index.search(q, top_k)  # D are inner products (since vectors normalized)
            return I, D
        elif self.method == "hnsw":
            # hnswlib.knn_query returns (ids, distances)
            ids, distances = self.index.knn_query(q, k=top_k)
            # distances are cosine distances (0..2). Convert to similarity = 1 - distance
            sim = 1.0 - distances
            return ids, sim
        else:
            # brute force dot product
            embs = self.index["embs"]  # shape (n, dim)
            sims = np.matmul(q, embs.T)  # (m, n)
            # get top_k
            idx = np.argpartition(-sims, range(min(top_k, sims.shape[1])), axis=1)[:, :top_k]
            # sort them
            row_idx = np.arange(sims.shape[0])[:, None]
            top_sims = sims[row_idx, idx]
            order = np.argsort(-top_sims, axis=1)
            sorted_idx = idx[row_idx, order]
            sorted_sims = np.take_along_axis(top_sims, order, axis=1)
            return sorted_idx, sorted_sims

# ---------------------------
# High-level matcher (with ANN candidate retrieval + rerank)
# ---------------------------
class LRRControlMatcherFull:
    def __init__(
        self,
        controls_df: pd.DataFrame,
        control_text_col: str = "Control Description",
        control_id_col: str = "ControlId",
        st_model_name: str = "all-MiniLM-L6-v2",
        st_batch_size: int = 128,
    ):
        self.controls_df = controls_df.copy().reset_index(drop=True)
        if control_text_col not in self.controls_df.columns:
            raise ValueError(f"controls_df must contain column '{control_text_col}'")
        self.control_text_col = control_text_col
        self.control_id_col = control_id_col if control_id_col in self.controls_df.columns else None
        self.controls_df["_txt"] = self.controls_df[self.control_text_col].fillna("").astype(str).apply(preprocess_text)
        self.st_model_name = st_model_name
        self.st_model = None
        self.st_batch_size = st_batch_size
        self.control_embs = None  # numpy array
        self.ann = ANNIndex()
        # For TF-IDF fallback
        self.tfidf_vectorizer = None
        self.svd = None
        self.tfidf_embs = None

    # ---------- embeddings ----------
    def load_or_build_control_embeddings(self, force_rebuild: bool = False, use_sentence_transformer: bool = True, show_progress: bool = True):
        texts = self.controls_df["_txt"].tolist()
        if use_sentence_transformer and SENTENCE_TRANSFORMERS_AVAILABLE:
            if self.control_embs is None or force_rebuild:
                if self.st_model is None:
                    self.st_model = get_st_model(self.st_model_name)
                self.control_embs = encode_texts_st(self.st_model, texts, batch_size=self.st_batch_size, show_progress=show_progress)
            return "st", self.control_embs
        # fallback
        Xr, vec, svd = tfidf_svd_embeddings(texts, n_components=min(200, max(1, 100)))
        self.tfidf_vectorizer = vec
        self.svd = svd
        self.tfidf_embs = Xr
        return "tfidf", Xr

    def save_control_embeddings(self, path_prefix: str):
        os.makedirs(os.path.dirname(path_prefix) or ".", exist_ok=True)
        if self.control_embs is not None:
            np.save(path_prefix + ".st_emb.npy", self.control_embs)
            meta = {"type": "st", "dim": int(self.control_embs.shape[1])}
            with open(path_prefix + ".meta.json", "w") as f:
                json.dump(meta, f)
        elif self.tfidf_embs is not None:
            np.save(path_prefix + ".tfidf_emb.npy", self.tfidf_embs)
            # vectorizer and svd objects are pickle-able
            pd.to_pickle(self.tfidf_vectorizer, path_prefix + ".tfidf_vectorizer.pkl")
            pd.to_pickle(self.svd, path_prefix + ".tfidf_svd.pkl")
            meta = {"type": "tfidf", "dim": int(self.tfidf_embs.shape[1])}
            with open(path_prefix + ".meta.json", "w") as f:
                json.dump(meta, f)
        else:
            raise ValueError("No embeddings to save")

    def load_control_embeddings(self, path_prefix: str):
        meta = json.load(open(path_prefix + ".meta.json", "r"))
        t = meta.get("type")
        if t == "st":
            self.control_embs = np.load(path_prefix + ".st_emb.npy")
        elif t == "tfidf":
            self.tfidf_embs = np.load(path_prefix + ".tfidf_emb.npy")
            self.tfidf_vectorizer = pd.read_pickle(path_prefix + ".tfidf_vectorizer.pkl")
            self.svd = pd.read_pickle(path_prefix + ".tfidf_svd.pkl")
        else:
            raise ValueError("Unknown embeddings type in meta")

    # ---------- ANN ----------
    def build_ann_index(self, method: str = "auto", use_gpu: bool = False, ef_construction: int = 200, M: int = 16):
        if self.control_embs is None:
            raise ValueError("control_embs missing: call load_or_build_control_embeddings(use_sentence_transformer=True) first")
        # choose method according to preference
        self.ann = ANNIndex(method_preference=method)
        chosen = self.ann.build(self.control_embs, ef_construction=ef_construction, M=M, use_gpu=use_gpu)
        return chosen

    def save_ann(self, path_prefix: str):
        self.ann.save(path_prefix)

    def load_ann(self, path_prefix: str):
        self.ann.load(path_prefix)

    # ---------- matching ----------
    def match_single_lrr(
        self,
        lrr_text: str,
        top_k: int = 3,
        candidate_pool: int = 50,
        methods: Optional[List[str]] = None,
        weights: Optional[Dict[str, float]] = None,
        use_sentence_transformer: bool = True,
    ) -> pd.DataFrame:
        """
        For a single LRR:
         - compute query embedding (ST or TFIDF fallback)
         - do ANN query to retrieve candidate_pool candidate controls
         - compute per-method scores on candidates
         - compute normalized composite and return top_k
        """
        if methods is None:
            methods = ["semantic", "ngram", "jaccard", "fuzzy", "regex", "phrase"]
        q = preprocess_text(lrr_text)
        n_controls = len(self.controls_df)

        # compute query embedding
        if use_sentence_transformer and SENTENCE_TRANSFORMERS_AVAILABLE and self.control_embs is not None:
            if self.st_model is None:
                self.st_model = get_st_model(self.st_model_name)
            q_emb = encode_texts_st(self.st_model, [q], batch_size=1, show_progress=False)[0]
            # retrieve candidate ids via ann
            if self.ann.method is None:
                # fallback to brute force search over control_embs
                sims = cosine_similarity_matrix(q_emb.reshape(1, -1), self.control_embs).flatten()
                cand_ids = np.argsort(-sims)[:candidate_pool]
                sem_scores_full = sims
            else:
                ids, sims = self.ann.query(q_emb.reshape(1, -1), top_k=min(candidate_pool, n_controls))
                cand_ids = ids[0]
                sem_scores_full = None
        else:
            # fallback TF-IDF approach: if tfidf_embs not computed compute them
            if self.tfidf_embs is None:
                _, _ = self.load_or_build_control_embeddings(use_sentence_transformer=False)
            # compute query tfidf embedding via fitted vectorizer + svd
            vec = self.tfidf_vectorizer
            svd = self.svd
            q_vec = vec.transform([q])
            q_emb = svd.transform(q_vec)
            sims = cosine_similarity_matrix(q_emb, self.tfidf_embs).flatten()
            cand_ids = np.argsort(-sims)[:candidate_pool]
            sem_scores_full = sims

        # prepare candidate subset
        cand_df = self.controls_df.iloc[cand_ids].copy().reset_index(drop=True)
        cand_texts = cand_df["_txt"].tolist()

        # compute semantic scores for candidates (if not available)
        if "semantic" in methods:
            if use_sentence_transformer and SENTENCE_TRANSFORMERS_AVAILABLE and self.control_embs is not None:
                # compute dot with precomputed control embeddings
                # find control_embs rows corresponding to cand_ids
                emb_cands = self.control_embs[cand_ids]
                sem_scores = cosine_similarity_matrix(q_emb.reshape(1, -1), emb_cands).flatten()
            else:
                # we already computed sims above for TF-IDF fallback
                sem_scores = sem_scores_full[cand_ids] if sem_scores_full is not None else np.zeros(len(cand_ids))
            cand_df["semantic"] = sem_scores

        # compute other methods
        if "ngram" in methods:
            cand_df["ngram"] = cand_df["_txt"].apply(lambda x: ngram_overlap_score(q, x))
        if "jaccard" in methods:
            cand_df["jaccard"] = cand_df["_txt"].apply(lambda x: jaccard_tokens(q, x))
        if "fuzzy" in methods:
            cand_df["fuzzy"] = cand_df["_txt"].apply(lambda x: fuzzy_ratio(q, x))
        if "regex" in methods:
            cand_df["regex"] = cand_df["_txt"].apply(lambda x: regex_section_match_score(q, x))
        if "phrase" in methods:
            cand_df["phrase"] = cand_df["_txt"].apply(lambda x: phrase_match_count(q, x))

        # normalize
        score_cols = [c for c in ["semantic", "ngram", "jaccard", "fuzzy", "regex", "phrase"] if c in cand_df.columns]
        for c in score_cols:
            vals = cand_df[c].fillna(0).values.astype(float)
            mn, mx = vals.min(), vals.max()
            if mx == mn:
                cand_df[c + "_norm"] = 0.0 if mx == 0 else 1.0
            else:
                cand_df[c + "_norm"] = (vals - mn) / (mx - mn)

        # weights
        if weights is None:
            weights = {"semantic": 3.0, "ngram": 1.0, "jaccard": 0.5, "fuzzy": 0.5, "regex": 2.0, "phrase": 1.0}
        comp = np.zeros(len(cand_df), dtype=float)
        total_w = 0.0
        for method, w in weights.items():
            col = method + "_norm"
            if col in cand_df.columns:
                comp += cand_df[col].astype(float).values * float(w)
                total_w += float(w)
        if total_w > 0:
            comp /= total_w
        cand_df["composite_score"] = comp

        top = cand_df.sort_values("composite_score", ascending=False).head(top_k).reset_index(drop=True)
        # prepare return columns
        out_cols = []
        if self.control_id_col:
            out_cols.append(self.control_id_col)
        out_cols += [self.control_text_col]
        out_cols += score_cols
        out_cols += [c + "_norm" for c in score_cols]
        out_cols += ["composite_score"]
        return top[[c for c in out_cols if c in top.columns]]

    def match_all_lrrs(
        self,
        laws_df: pd.DataFrame,
        lrr_id_col: str = "LRRid",
        lrr_text_col: str = "Detailed obligation description",
        top_k: int = 3,
        candidate_pool: int = 50,
        methods: Optional[List[str]] = None,
        weights: Optional[Dict[str, float]] = None,
        use_sentence_transformer: bool = True,
        batch_size: int = 64,
        show_progress: bool = True,
    ) -> pd.DataFrame:
        """
        Loop over all LRRs and call match_single_lrr for each. Batch-mode for embeddings is handled elsewhere.
        Returns a long-format DataFrame with top_k rows per LRR.
        """
        if methods is None:
            methods = ["semantic", "ngram", "jaccard", "fuzzy", "regex", "phrase"]
        laws = laws_df.copy().reset_index(drop=True)
        if lrr_text_col not in laws.columns:
            raise ValueError(f"laws_df must contain column '{lrr_text_col}'")
        laws["_txt"] = laws[lrr_text_col].fillna("").astype(str).apply(preprocess_text)

        rows = []
        it_range = range(0, len(laws))
        if TQDM_AVAILABLE and show_progress:
            it_range = tqdm(it_range, desc="Matching LRRs", file=sys.stdout)

        for i in it_range:
            lrrid = laws.at[i, lrr_id_col] if lrr_id_col in laws.columns else i
            lrr_text = laws.at[i, "_txt"]
            top = self.match_single_lrr(
                lrr_text,
                top_k=top_k,
                candidate_pool=candidate_pool,
                methods=methods,
                weights=weights,
                use_sentence_transformer=use_sentence_transformer,
            )
            for rank_idx, t in top.reset_index(drop=True).iterrows():
                out = {lrr_id_col: lrrid, lrr_text_col: lrr_text, "rank": int(rank_idx) + 1}
                if self.control_id_col:
                    out[self.control_id_col] = t.get(self.control_id_col)
                out[self.control_text_col] = t.get(self.control_text_col)
                for c in [c for c in top.columns if c not in [self.control_id_col, self.control_text_col]]:
                    out[c] = t[c]
                rows.append(out)
        return pd.DataFrame(rows)

# ---------------------------
# CLI helpers
# ---------------------------
def read_table_auto(path: str) -> pd.DataFrame:
    _, ext = os.path.splitext(path.lower())
    if ext in [".xlsx", ".xls"]:
        return pd.read_excel(path)
    else:
        return pd.read_csv(path)

def main():
    parser = argparse.ArgumentParser(description="LRR to Control matcher with sentence-transformers + ANN (faiss/hnswlib).")
    parser.add_argument("--laws", type=str, help="CSV/XLSX with LRRs (columns: LRRid, Detailed obligation description)")
    parser.add_argument("--controls", type=str, help="CSV/XLSX with controls (columns: ControlId, Control Description)")
    parser.add_argument("--output", type=str, default="lrr_control_matches.csv", help="Output CSV path")
    parser.add_argument("--model", type=str, default="all-MiniLM-L6-v2", help="SentenceTransformer model")
    parser.add_argument("--use-st", action="store_true", help="Use sentence-transformers for embeddings (recommended)")
    parser.add_argument("--build-emb", type=str, help="Path prefix to save control embeddings (e.g., data/controls_emb). Saves .st_emb.npy/.meta.json")
    parser.add_argument("--load-emb", type=str, help="Path prefix to load control embeddings")
    parser.add_argument("--build-ann", type=str, help="Path prefix to save ANN index (e.g., data/controls_index)")
    parser.add_argument("--load-ann", type=str, help="Path prefix to load ANN index")
    parser.add_argument("--ann-method", type=str, choices=["auto", "faiss", "hnsw", "brute"], default="auto", help="ANN method preference")
    parser.add_argument("--use-faiss-gpu", action="store_true", help="Attempt to use FAISS GPU (if faiss installed with GPU support)")
    parser.add_argument("--candidate-pool", type=int, default=50, help="Candidate pool size retrieved from ANN before reranking")
    parser.add_argument("--top-k", type=int, default=3, help="Top K controls per LRR to output")
    parser.add_argument("--batch-size", type=int, default=64, help="Batch size for encoding LRRs/controls")
    parser.add_argument("--no-progress", action="store_true", help="Disable progress bars")
    args = parser.parse_args()

    show_progress = not args.no_progress

    if not args.controls:
        print("Provide --controls path to controls CSV/XLSX (has Control Description column). Running demo instead.\n")
        # demo data
        demo_laws = pd.DataFrame([
            {"LRRid": "LRR001", "Detailed obligation description": """
                LYOD is not required to provide customers with notifications of changes that are increases to APRs or fees upon the expiration of a specified period of time,
                provided LYOD has satisfied the conditions laid out in 12 CFR 1026.9(c)(2)(v)(B). For increases in rates due to delinquency or default, LYOD must provide a written notice
                at least 45 days in advance with reasons and affected balances.
            """}
        ])
        demo_controls = pd.DataFrame([
            {"ControlId": "C001", "Control Description": "Notify customers in writing of APR changes after promotional period ends as per 12 CFR 1026.9(c)(2)(v)(B). 45 days notice for default rates."},
            {"ControlId": "C002", "Control Description": "System validates APR calculations and posts changes to customer accounts. No specific regulatory citation."},
            {"ControlId": "C003", "Control Description": "Customer communications team to send statements of rate changes. Includes reasons and affected balances; follows 12 CFR 1026.9(g)(3)(ii) formatting."},
            {"ControlId": "C004", "Control Description": "Password rotation and IT access controls unrelated to APR or consumer notices."}
        ])
        matcher = LRRControlMatcherFull(demo_controls, st_model_name=args.model, st_batch_size=args.batch_size)
        if args.use_st:
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                print("sentence-transformers not available. Install with: pip install -U sentence-transformers")
                return
            matcher.load_or_build_control_embeddings(force_rebuild=True, use_sentence_transformer=True, show_progress=show_progress)
            # build index automatically
            matcher.build_ann_index(method=args.ann_method, use_gpu=args.use_faiss_gpu)
        else:
            matcher.load_or_build_control_embeddings(force_rebuild=True, use_sentence_transformer=False, show_progress=show_progress)
        matches = matcher.match_all_lrrs(demo_laws, top_k=args.top_k, candidate_pool=args.candidate_pool, use_sentence_transformer=args.use_st, batch_size=args.batch_size, show_progress=show_progress)
        print(matches.to_string(index=False))
        matches.to_csv(args.output, index=False)
        print(f"Saved demo results to {args.output}")
        return

    # real data path
    controls_df = read_table_auto(args.controls)
    # validate presence
    if "Control Description" not in controls_df.columns:
        print("ERROR: controls file must contain column 'Control Description'")
        return

    matcher = LRRControlMatcherFull(controls_df, st_model_name=args.model, st_batch_size=args.batch_size)

    # load or build control embeddings
    if args.load_emb:
        matcher.load_control_embeddings(args.load_emb)
        print(f"Loaded embeddings from {args.load_emb}")
    else:
        st_flag = args.use_st and SENTENCE_TRANSFORMERS_AVAILABLE
        if args.use_st and not SENTENCE_TRANSFORMERS_AVAILABLE:
            print("Warning: sentence-transformers requested but not available. Falling back to TF-IDF+SVD.")
            st_flag = False
        typ, emb = matcher.load_or_build_control_embeddings(force_rebuild=True, use_sentence_transformer=st_flag, show_progress=show_progress)
        print("Built control embeddings (type=%s, shape=%s)" % (typ, emb.shape if isinstance(emb, np.ndarray) else "n/a"))
        if args.build_emb:
            matcher.save_control_embeddings(args.build_emb)
            print("Saved embeddings to", args.build_emb)

    # build or load ANN
    if args.load_ann:
        matcher.load_ann(args.load_ann)
        print("Loaded ANN index from", args.load_ann)
    else:
        if args.build_ann:
            # need control_embs present
            if matcher.control_embs is None:
                print("ERROR: ANN building requires sentence-transformer control embeddings. Use --use-st when building embeddings.")
            else:
                chosen = matcher.build_ann_index(method=args.ann_method, use_gpu=args.use_faiss_gpu)
                matcher.save_ann(args.build_ann)
                print(f"Built ANN index (method={chosen}) and saved to {args.build_ann}")
        else:
            # no ann requested; proceed without it (brute force will be used inside matching)
            pass

    # load laws
    if not args.laws:
        print("ERROR: provide --laws path with LRR dataset")
        return
    laws_df = read_table_auto(args.laws)
    if "Detailed obligation description" not in laws_df.columns:
        print("ERROR: laws file must contain column 'Detailed obligation description'")
        return

    # run matching
    matches = matcher.match_all_lrrs(laws_df, top_k=args.top_k, candidate_pool=args.candidate_pool, use_sentence_transformer=args.use_st, batch_size=args.batch_size, show_progress=show_progress)
    matches.to_csv(args.output, index=False)
    print(f"Saved matches to {args.output}")

if __name__ == "__main__":
    main()