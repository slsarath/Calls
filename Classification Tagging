"""
fill_materiality_no_calib.py

Usage example:
python fill_materiality_no_calib.py --input_xlsx data.xlsx \
  --text_col "call transcripts" --label_col "materiality" \
  --model "sentence-transformers/all-mpnet-base-v2" --method prototype --threshold 0.75

Requirements:
  pip install sentence-transformers transformers scikit-learn pandas numpy openpyxl tqdm
"""

import argparse
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import torch

# ----------------------------
# Defaults
# ----------------------------
DEFAULT_MODEL = "sentence-transformers/all-mpnet-base-v2"
DEFAULT_METHOD = "prototype"   # "prototype" or "knn"
DEFAULT_K = 5
DEFAULT_THRESHOLD = 0.75
BATCH_SIZE = 8
RANDOM_STATE = 42
SAMPLE_SIZE = 30  # how many top predicted blanks to include in sample sheet

# ----------------------------
# Helpers
# ----------------------------
def normalise_label(s):
    """Turn raw labels into 'COMPLAINT' / 'CONCERN' or None"""
    if pd.isna(s):
        return None
    s = str(s).strip().upper()
    if s.startswith("COMPL"):
        return "COMPLAINT"
    if s.startswith("CONC"):
        return "CONCERN"
    return None

def tokenize_to_id_chunks(tokenizer, text, chunk_size):
    """Return list of token-id chunks for text (no special tokens)."""
    if not isinstance(text, str) or text.strip() == "":
        return []
    ids = tokenizer.encode(text, add_special_tokens=False)
    if len(ids) == 0:
        return []
    return [ids[i:i+chunk_size] for i in range(0, len(ids), chunk_size)]

def decode_id_chunk(tokenizer, id_chunk):
    return tokenizer.decode(id_chunk, skip_special_tokens=True, clean_up_tokenization_spaces=True)

def encode_long_text(model, tokenizer, text, chunk_size):
    """
    Encode a long text by token-id chunking + decode + encode(chunks) + mean-pool.
    Returns a normalized numpy vector (dim,).
    """
    # safe empty
    if not isinstance(text, str) or text.strip() == "":
        return np.zeros(model.get_sentence_embedding_dimension(), dtype=float)

    id_chunks = tokenize_to_id_chunks(tokenizer, text, chunk_size)
    if not id_chunks:
        return np.zeros(model.get_sentence_embedding_dimension(), dtype=float)

    text_chunks = [decode_id_chunk(tokenizer, c) for c in id_chunks]

    # encode chunks (convert_to_tensor -> mean pool on GPU/CPU)
    with torch.no_grad():
        chunk_embs = model.encode(
            text_chunks,
            batch_size=BATCH_SIZE,
            convert_to_tensor=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        pooled = torch.mean(chunk_embs, dim=0)
        pooled = pooled / (pooled.norm(p=2) + 1e-9)
        return pooled.cpu().numpy()

def build_centroids(embeddings, labels):
    """
    embeddings: np.array (n x d)
    labels: list/array of strings ("COMPLAINT"/"CONCERN")
    """
    lab_arr = np.array(labels)
    centroids = {}
    for lab in np.unique(lab_arr):
        mask = (lab_arr == lab)
        if mask.sum() == 0:
            continue
        centroid = embeddings[mask].mean(axis=0)
        norm = np.linalg.norm(centroid) + 1e-9
        centroids[lab] = centroid / norm
    return centroids

def predict_prototype(centroids, emb):
    sims = {lab: float(np.dot(centroids[lab], emb)) for lab in centroids}
    best_lab = max(sims, key=sims.get)
    return best_lab, sims[best_lab], sims

def knn_predict(labeled_embs, labeled_labels, emb, k=5):
    sims = (labeled_embs @ emb)  # dot product of normalized vectors = cosine
    top_idx = np.argsort(-sims)[:k]
    top_labels = np.array(labeled_labels)[top_idx]
    unique, counts = np.unique(top_labels, return_counts=True)
    majority_label = unique[np.argmax(counts)]
    top_sim = float(sims[top_idx[0]])
    return majority_label, top_sim, top_labels.tolist()

# ----------------------------
# Main function
# ----------------------------
def fill_blanks_from_excel_no_calib(
    input_xlsx,
    text_col="call transcripts",
    label_col="materiality",
    model_name=DEFAULT_MODEL,
    method=DEFAULT_METHOD,
    k=DEFAULT_K,
    threshold=DEFAULT_THRESHOLD,
    output_xlsx="materiality_filled.xlsx",
    sample_size=SAMPLE_SIZE
):
    # Read Excel (single sheet)
    df = pd.read_excel(input_xlsx)
    if text_col not in df.columns or label_col not in df.columns:
        raise ValueError(f"Columns not found. Available: {list(df.columns)}")

    df = df.copy()
    df["_label_norm"] = df[label_col].apply(normalise_label)

    labeled_df = df[df["_label_norm"].notnull()].reset_index(drop=True)
    unlabeled_df = df[df["_label_norm"].isnull()].reset_index(drop=True)

    print(f"Total rows: {len(df)} | Labeled: {len(labeled_df)} | Blanks: {len(unlabeled_df)}")

    # Load model + tokenizer
    print(f"Loading model: {model_name}")
    model = SentenceTransformer(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

    # chunk size derived from tokenizer max length
    model_max = tokenizer.model_max_length if hasattr(tokenizer, "model_max_length") else 512
    chunk_size = max(64, model_max - 10)
    print(f"Tokenizer max length: {model_max}; using chunk_size token ids = {chunk_size}")

    # Encode labeled with chunk pooling
    print("Encoding labeled transcripts (chunking + pooling)...")
    labeled_texts = labeled_df[text_col].fillna("").astype(str).tolist()
    labeled_embs = []
    for t in tqdm(labeled_texts, desc="encode labeled"):
        labeled_embs.append(encode_long_text(model, tokenizer, t, chunk_size))
    labeled_embs = np.vstack(labeled_embs) if len(labeled_embs) else np.zeros((0, model.get_sentence_embedding_dimension()))
    labeled_labels = labeled_df["_label_norm"].tolist()

    # Build centroids if prototype
    centroids = None
    if method == "prototype":
        centroids = build_centroids(labeled_embs, labeled_labels)

    # Encode unlabeled and predict
    print("Encoding blank transcripts and predicting...")
    unl_texts = unlabeled_df[text_col].fillna("").astype(str).tolist()
    preds = []
    confs = []
    methods = []
    unl_embs = []
    for t in tqdm(unl_texts, desc="encode+predict blanks"):
        emb = encode_long_text(model, tokenizer, t, chunk_size)
        unl_embs.append(emb)
        if method == "prototype":
            lab, sim, sims = predict_prototype(centroids, emb)
        else:
            lab, sim, top_labels = knn_predict(labeled_embs, labeled_labels, emb, k=k)
        pred = lab if sim >= threshold else "NEUTRAL"
        preds.append(pred)
        confs.append(sim)
        methods.append(method)

    unlabeled_df = unlabeled_df.copy()
    unlabeled_df["pred_materiality"] = preds
    unlabeled_df["pred_confidence"] = confs
    unlabeled_df["pred_method"] = methods

    # Merge back into full df
    df_result = df.copy()
    df_result["pred_materiality"] = df_result["_label_norm"]
    blank_idx = df_result[df_result["_label_norm"].isnull()].index
    df_result.loc[blank_idx, "pred_materiality"] = unlabeled_df["pred_materiality"].values

    df_result["final_materiality"] = df_result.apply(
        lambda r: r["_label_norm"] if pd.notna(r["_label_norm"]) else (r["pred_materiality"] if pd.notna(r["pred_materiality"]) else "NEUTRAL"),
        axis=1
    )
    df_result["final_materiality"] = df_result["final_materiality"].fillna("NEUTRAL").astype(str)

    print("\nCounts after fill:")
    print(df_result["final_materiality"].value_counts())

    # Save to Excel with extra sheet for top N predicted blanks
    out_name = output_xlsx
    try:
        with pd.ExcelWriter(out_name, engine="openpyxl") as writer:
            df_result.to_excel(writer, sheet_name="all_results", index=False)
            # top predicted blanks sample by confidence
            sample_df = unlabeled_df.copy()
            sample_df[text_col] = unl_texts
            sample_sorted = sample_df.sort_values("pred_confidence", ascending=False).head(sample_size)
            sample_sorted.to_excel(writer, sheet_name="predicted_blanks_sample", index=False)
        print(f"Saved results and sample to {out_name}")
    except Exception as e:
        # fallback to CSV if Excel write fails
        print(f"Excel write failed ({e}); falling back to CSV.")
        df_result.to_csv(out_name.replace(".xlsx", ".csv"), index=False)
        unlabeled_df.sort_values("pred_confidence", ascending=False).head(sample_size).to_csv(
            out_name.replace(".xlsx", "_sample.csv"), index=False
        )
        print("Saved CSV outputs.")

    # Print short sample to console
    print("\nTop predicted blanks (console sample):")
    sample_console = unlabeled_df.copy()
    sample_console[text_col] = unl_texts
    print(sample_console.sort_values("pred_confidence", ascending=False).head(10)[[text_col, "pred_materiality", "pred_confidence"]].to_string(index=False))

    return df_result, unlabeled_df

# ----------------------------
# CLI
# ----------------------------
if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--input_xlsx", default="data.xlsx", help="Input Excel file (single sheet)")
    p.add_argument("--text_col", default="call transcripts")
    p.add_argument("--label_col", default="materiality")
    p.add_argument("--model", default=DEFAULT_MODEL)
    p.add_argument("--method", choices=["prototype", "knn"], default=DEFAULT_METHOD)
    p.add_argument("--k", type=int, default=DEFAULT_K)
    p.add_argument("--threshold", type=float, default=DEFAULT_THRESHOLD,
                   help="Similarity threshold (no calibration in this script)")
    p.add_argument("--output_xlsx", default="materiality_filled.xlsx")
    args = p.parse_args()

    fill_blanks_from_excel_no_calib(
        input_xlsx=args.input_xlsx,
        text_col=args.text_col,
        label_col=args.label_col,
        model_name=args.model,
        method=args.method,
        k=args.k,
        threshold=args.threshold,
        output_xlsx=args.output_xlsx,
        sample_size=SAMPLE_SIZE
    )